
==> Audit <==
|--------------|-----------------------|----------|-------------------|---------|---------------------|---------------------|
|   Command    |         Args          | Profile  |       User        | Version |     Start Time      |      End Time       |
|--------------|-----------------------|----------|-------------------|---------|---------------------|---------------------|
| update-check |                       | minikube | CORPROOT\TAAFAAL4 | v1.34.0 | 20 Jan 25 13:46 CET | 20 Jan 25 13:46 CET |
| start        | --driver=docker       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:18 CET | 18 Feb 25 14:21 CET |
| addons       | enable ingress        | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:22 CET | 18 Feb 25 14:22 CET |
| addons       | enable dashboard      | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:22 CET | 18 Feb 25 14:22 CET |
| addons       | enable metrics-server | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:23 CET | 18 Feb 25 14:23 CET |
| addons       | enable ingress        | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:23 CET | 18 Feb 25 14:23 CET |
| dashboard    | --url                 | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:24 CET |                     |
| start        | --driver=docker       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 18 Feb 25 14:25 CET | 18 Feb 25 14:25 CET |
| update-check |                       | minikube | CORPROOT\TAAFAAL4 | v1.34.0 | 18 Feb 25 16:15 CET | 18 Feb 25 16:15 CET |
| update-check |                       | minikube | CORPROOT\TAAFAAL4 | v1.34.0 | 19 Feb 25 08:29 CET | 19 Feb 25 08:29 CET |
| update-check |                       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 08:54 CET | 19 Feb 25 08:54 CET |
| start        | --driver=docker       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 08:56 CET |                     |
| dashboard    | url                   | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 08:56 CET |                     |
| update-check |                       | minikube | CORPROOT\TAAFAAL4 | v1.34.0 | 19 Feb 25 09:11 CET | 19 Feb 25 09:11 CET |
| dashboard    | --url                 | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 09:16 CET |                     |
| start        | --driver=docker       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 09:16 CET | 19 Feb 25 09:16 CET |
| dashboard    | --url                 | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 09:16 CET |                     |
| ip           |                       | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 09:19 CET | 19 Feb 25 09:19 CET |
| service      | webapp-service        | minikube | CORPROOT\TAAFAAL4 | v1.35.0 | 19 Feb 25 09:20 CET |                     |
|--------------|-----------------------|----------|-------------------|---------|---------------------|---------------------|


==> Letzter Start <==
Log file created at: 2025/02/19 09:16:13
Running on machine: U342639
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0219 09:16:13.260450    7192 out.go:345] Setting OutFile to fd 96 ...
I0219 09:16:13.294238    7192 out.go:358] Setting ErrFile to fd 100...
W0219 09:16:13.312191    7192 root.go:314] Error reading config file at C:\Users\TAAFAAL4\.minikube\config\config.json: open C:\Users\TAAFAAL4\.minikube\config\config.json: The system cannot find the file specified.
I0219 09:16:13.320899    7192 out.go:352] Setting JSON to false
I0219 09:16:13.326032    7192 start.go:129] hostinfo: {"hostname":"U342639","uptime":504,"bootTime":1739952469,"procs":386,"os":"windows","platform":"Microsoft Windows 11 Enterprise","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.4751 Build 22621.4751","kernelVersion":"10.0.22621.4751 Build 22621.4751","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"24abcbba-db15-40b6-ab92-fa56faaa9482"}
W0219 09:16:13.326032    7192 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0219 09:16:13.328635    7192 out.go:177] 😄  minikube v1.35.0 auf Microsoft Windows 11 Enterprise 10.0.22621.4751 Build 22621.4751
I0219 09:16:13.330346    7192 notify.go:220] Checking for updates...
I0219 09:16:13.332056    7192 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0219 09:16:13.333878    7192 driver.go:394] Setting default libvirt URI to qemu:///system
I0219 09:16:13.426512    7192 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0219 09:16:13.434676    7192 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 09:16:18.362558    7192 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.9278827s)
I0219 09:16:18.363165    7192 info.go:266] docker info: {ID:ee88b893-520f-4c2f-94eb-7c940c16d874 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:70 OomKillDisable:true NGoroutines:91 SystemTime:2025-02-19 08:16:18.362148223 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16619945984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0219 09:16:18.365256    7192 out.go:177] ✨  Verwende den Treiber docker basierend auf dem existierenden Profil
I0219 09:16:18.366315    7192 start.go:297] selected driver: docker
I0219 09:16:18.366315    7192 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\TAAFAAL4:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0219 09:16:18.366843    7192 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0219 09:16:18.377843    7192 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 09:16:18.659424    7192 info.go:266] docker info: {ID:ee88b893-520f-4c2f-94eb-7c940c16d874 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:true NGoroutines:89 SystemTime:2025-02-19 08:16:18.647777295 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:19 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16619945984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\TAAFAAL4\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0219 09:16:18.707469    7192 cni.go:84] Creating CNI manager for ""
I0219 09:16:18.707469    7192 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0219 09:16:18.707469    7192 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\TAAFAAL4:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0219 09:16:18.709039    7192 out.go:177] 👍  Starte "minikube" primary control-plane Node im "minikube" Cluster
I0219 09:16:18.710630    7192 cache.go:121] Beginning downloading kic base image for docker with docker
I0219 09:16:18.711694    7192 out.go:177] 🚜  Ziehe Base Image v0.0.46 ...
I0219 09:16:18.713360    7192 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0219 09:16:18.713360    7192 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0219 09:16:18.713865    7192 preload.go:146] Found local preload: C:\Users\TAAFAAL4\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0219 09:16:18.713865    7192 cache.go:56] Caching tarball of preloaded images
I0219 09:16:18.713865    7192 preload.go:172] Found C:\Users\TAAFAAL4\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0219 09:16:18.713865    7192 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0219 09:16:18.714383    7192 profile.go:143] Saving config to C:\Users\TAAFAAL4\.minikube\profiles\minikube\config.json ...
I0219 09:16:18.778207    7192 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0219 09:16:18.778207    7192 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0219 09:16:18.778207    7192 cache.go:227] Successfully downloaded all kic artifacts
I0219 09:16:18.778207    7192 start.go:360] acquireMachinesLock for minikube: {Name:mk6a92eaeeb7cfe21d3795e116db48aa5d90f4f9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0219 09:16:18.778207    7192 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0219 09:16:18.778720    7192 start.go:96] Skipping create...Using existing machine configuration
I0219 09:16:18.778720    7192 fix.go:54] fixHost starting: 
I0219 09:16:18.789447    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:18.829559    7192 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0219 09:16:18.829559    7192 fix.go:138] unexpected machine state, will restart: <nil>
I0219 09:16:18.830603    7192 out.go:177] 🏃  Aktualisiere den laufenden docker "minikube" container ...
I0219 09:16:18.832678    7192 machine.go:93] provisionDockerMachine start ...
I0219 09:16:18.837879    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:18.878518    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:18.879034    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:18.879034    7192 main.go:141] libmachine: About to run SSH command:
hostname
I0219 09:16:19.026538    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0219 09:16:19.026538    7192 ubuntu.go:169] provisioning hostname "minikube"
I0219 09:16:19.034452    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:19.083748    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:19.083748    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:19.083748    7192 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0219 09:16:19.247604    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0219 09:16:19.253937    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:19.299081    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:19.299601    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:19.299601    7192 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0219 09:16:19.433278    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0219 09:16:19.433278    7192 ubuntu.go:175] set auth options {CertDir:C:\Users\TAAFAAL4\.minikube CaCertPath:C:\Users\TAAFAAL4\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\TAAFAAL4\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\TAAFAAL4\.minikube\machines\server.pem ServerKeyPath:C:\Users\TAAFAAL4\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\TAAFAAL4\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\TAAFAAL4\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\TAAFAAL4\.minikube}
I0219 09:16:19.433278    7192 ubuntu.go:177] setting up certificates
I0219 09:16:19.433278    7192 provision.go:84] configureAuth start
I0219 09:16:19.440117    7192 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0219 09:16:19.491408    7192 provision.go:143] copyHostCerts
I0219 09:16:19.504737    7192 exec_runner.go:144] found C:\Users\TAAFAAL4\.minikube/ca.pem, removing ...
I0219 09:16:19.504737    7192 exec_runner.go:203] rm: C:\Users\TAAFAAL4\.minikube\ca.pem
I0219 09:16:19.505329    7192 exec_runner.go:151] cp: C:\Users\TAAFAAL4\.minikube\certs\ca.pem --> C:\Users\TAAFAAL4\.minikube/ca.pem (1082 bytes)
I0219 09:16:19.528922    7192 exec_runner.go:144] found C:\Users\TAAFAAL4\.minikube/cert.pem, removing ...
I0219 09:16:19.528922    7192 exec_runner.go:203] rm: C:\Users\TAAFAAL4\.minikube\cert.pem
I0219 09:16:19.528922    7192 exec_runner.go:151] cp: C:\Users\TAAFAAL4\.minikube\certs\cert.pem --> C:\Users\TAAFAAL4\.minikube/cert.pem (1127 bytes)
I0219 09:16:19.545409    7192 exec_runner.go:144] found C:\Users\TAAFAAL4\.minikube/key.pem, removing ...
I0219 09:16:19.545409    7192 exec_runner.go:203] rm: C:\Users\TAAFAAL4\.minikube\key.pem
I0219 09:16:19.545930    7192 exec_runner.go:151] cp: C:\Users\TAAFAAL4\.minikube\certs\key.pem --> C:\Users\TAAFAAL4\.minikube/key.pem (1675 bytes)
I0219 09:16:19.546519    7192 provision.go:117] generating server cert: C:\Users\TAAFAAL4\.minikube\machines\server.pem ca-key=C:\Users\TAAFAAL4\.minikube\certs\ca.pem private-key=C:\Users\TAAFAAL4\.minikube\certs\ca-key.pem org=TAAFAAL4.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0219 09:16:19.873257    7192 provision.go:177] copyRemoteCerts
I0219 09:16:19.889160    7192 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0219 09:16:19.894960    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:19.941129    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:20.050934    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0219 09:16:20.073518    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\machines\server.pem --> /etc/docker/server.pem (1184 bytes)
I0219 09:16:20.090469    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0219 09:16:20.107858    7192 provision.go:87] duration metric: took 674.5801ms to configureAuth
I0219 09:16:20.107858    7192 ubuntu.go:193] setting minikube options for container-runtime
I0219 09:16:20.108923    7192 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0219 09:16:20.114277    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:20.161846    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:20.162350    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:20.162350    7192 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0219 09:16:20.311392    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0219 09:16:20.311392    7192 ubuntu.go:71] root file system type: overlay
I0219 09:16:20.311392    7192 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0219 09:16:20.316770    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:20.365651    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:20.365651    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:20.365651    7192 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0219 09:16:20.504578    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0219 09:16:20.512025    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:20.566438    7192 main.go:141] libmachine: Using SSH client type: native
I0219 09:16:20.566438    7192 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12c5360] 0x12c7ea0 <nil>  [] 0s} 127.0.0.1 60104 <nil> <nil>}
I0219 09:16:20.566438    7192 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0219 09:16:20.697335    7192 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0219 09:16:20.697335    7192 machine.go:96] duration metric: took 1.8646576s to provisionDockerMachine
I0219 09:16:20.697335    7192 start.go:293] postStartSetup for "minikube" (driver="docker")
I0219 09:16:20.697335    7192 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0219 09:16:20.708116    7192 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0219 09:16:20.714055    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:20.758029    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:20.865589    7192 ssh_runner.go:195] Run: cat /etc/os-release
I0219 09:16:20.869722    7192 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0219 09:16:20.869722    7192 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0219 09:16:20.869722    7192 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0219 09:16:20.869722    7192 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0219 09:16:20.869722    7192 filesync.go:126] Scanning C:\Users\TAAFAAL4\.minikube\addons for local assets ...
I0219 09:16:20.870241    7192 filesync.go:126] Scanning C:\Users\TAAFAAL4\.minikube\files for local assets ...
I0219 09:16:20.870241    7192 start.go:296] duration metric: took 172.9054ms for postStartSetup
I0219 09:16:20.879918    7192 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0219 09:16:20.885154    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:20.928049    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:21.025000    7192 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0219 09:16:21.029712    7192 fix.go:56] duration metric: took 2.2509922s for fixHost
I0219 09:16:21.029712    7192 start.go:83] releasing machines lock for "minikube", held for 2.2509922s
I0219 09:16:21.034973    7192 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0219 09:16:21.080619    7192 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0219 09:16:21.087064    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:21.088122    7192 ssh_runner.go:195] Run: cat /version.json
I0219 09:16:21.095066    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:21.138552    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:21.147064    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
W0219 09:16:21.242781    7192 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0219 09:16:21.268390    7192 ssh_runner.go:195] Run: systemctl --version
I0219 09:16:21.286631    7192 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0219 09:16:21.301541    7192 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0219 09:16:21.311072    7192 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0219 09:16:21.321100    7192 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0219 09:16:21.329210    7192 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0219 09:16:21.329210    7192 start.go:495] detecting cgroup driver to use...
I0219 09:16:21.329210    7192 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0219 09:16:21.329210    7192 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0219 09:16:21.351765    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0219 09:16:21.371478    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0219 09:16:21.380410    7192 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0219 09:16:21.392579    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0219 09:16:21.412945    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0219 09:16:21.430652    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
W0219 09:16:21.454372    7192 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0219 09:16:21.454372    7192 out.go:270] 💡  Um neue externe Images zu ziehen, müsste eventuell ein Proxy konfiguriert werden: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0219 09:16:21.455955    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0219 09:16:21.480960    7192 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0219 09:16:21.501467    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0219 09:16:21.520941    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0219 09:16:21.541313    7192 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0219 09:16:21.566026    7192 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0219 09:16:21.588312    7192 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0219 09:16:21.609115    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:21.708770    7192 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0219 09:16:21.829480    7192 start.go:495] detecting cgroup driver to use...
I0219 09:16:21.829542    7192 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0219 09:16:21.846088    7192 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0219 09:16:21.858810    7192 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0219 09:16:21.882884    7192 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0219 09:16:21.901502    7192 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0219 09:16:21.935946    7192 ssh_runner.go:195] Run: which cri-dockerd
I0219 09:16:21.954504    7192 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0219 09:16:21.963248    7192 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0219 09:16:21.990777    7192 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0219 09:16:22.056817    7192 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0219 09:16:22.134810    7192 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0219 09:16:22.134810    7192 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0219 09:16:22.160466    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:22.259541    7192 ssh_runner.go:195] Run: sudo systemctl restart docker
I0219 09:16:26.699064    7192 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.4395228s)
I0219 09:16:26.708749    7192 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0219 09:16:26.727445    7192 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0219 09:16:26.746393    7192 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0219 09:16:26.765443    7192 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0219 09:16:26.881559    7192 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0219 09:16:26.998725    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:27.091382    7192 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0219 09:16:27.113307    7192 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0219 09:16:27.132639    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:27.256931    7192 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0219 09:16:27.453262    7192 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0219 09:16:27.464882    7192 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0219 09:16:27.469020    7192 start.go:563] Will wait 60s for crictl version
I0219 09:16:27.479351    7192 ssh_runner.go:195] Run: which crictl
I0219 09:16:27.495943    7192 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0219 09:16:27.613634    7192 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0219 09:16:27.621517    7192 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 09:16:27.723185    7192 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 09:16:27.745240    7192 out.go:235] 🐳  Vorbereiten von Kubernetes v1.32.0 auf Docker 27.4.1...
I0219 09:16:27.753690    7192 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0219 09:16:27.855998    7192 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0219 09:16:27.867564    7192 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0219 09:16:27.871854    7192 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0219 09:16:27.889299    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 09:16:27.929844    7192 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\TAAFAAL4:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0219 09:16:27.929844    7192 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0219 09:16:27.935713    7192 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 09:16:27.956582    7192 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0219 09:16:27.956582    7192 docker.go:619] Images already preloaded, skipping extraction
I0219 09:16:27.962310    7192 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 09:16:27.979658    7192 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0219 09:16:27.979658    7192 cache_images.go:84] Images are preloaded, skipping loading
I0219 09:16:27.979658    7192 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0219 09:16:27.979658    7192 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0219 09:16:27.987268    7192 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0219 09:16:28.123802    7192 cni.go:84] Creating CNI manager for ""
I0219 09:16:28.124318    7192 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0219 09:16:28.124318    7192 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0219 09:16:28.124318    7192 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0219 09:16:28.124318    7192 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0219 09:16:28.133824    7192 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0219 09:16:28.141537    7192 binaries.go:44] Found k8s binaries, skipping transfer
I0219 09:16:28.151215    7192 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0219 09:16:28.158461    7192 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0219 09:16:28.170944    7192 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0219 09:16:28.182318    7192 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0219 09:16:28.204565    7192 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0219 09:16:28.208699    7192 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0219 09:16:28.226166    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:28.320382    7192 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0219 09:16:28.331721    7192 certs.go:68] Setting up C:\Users\TAAFAAL4\.minikube\profiles\minikube for IP: 192.168.49.2
I0219 09:16:28.331721    7192 certs.go:194] generating shared ca certs ...
I0219 09:16:28.331721    7192 certs.go:226] acquiring lock for ca certs: {Name:mk976a28ee4d65eaf3f893728720c47f7cad4ca3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 09:16:28.342329    7192 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\TAAFAAL4\.minikube\ca.key
I0219 09:16:28.363212    7192 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\TAAFAAL4\.minikube\proxy-client-ca.key
I0219 09:16:28.363212    7192 certs.go:256] generating profile certs ...
I0219 09:16:28.364240    7192 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\TAAFAAL4\.minikube\profiles\minikube\client.key
I0219 09:16:28.392284    7192 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\TAAFAAL4\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0219 09:16:28.414247    7192 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\TAAFAAL4\.minikube\profiles\minikube\proxy-client.key
I0219 09:16:28.417419    7192 certs.go:484] found cert: C:\Users\TAAFAAL4\.minikube\certs\ca-key.pem (1675 bytes)
I0219 09:16:28.418462    7192 certs.go:484] found cert: C:\Users\TAAFAAL4\.minikube\certs\ca.pem (1082 bytes)
I0219 09:16:28.418976    7192 certs.go:484] found cert: C:\Users\TAAFAAL4\.minikube\certs\cert.pem (1127 bytes)
I0219 09:16:28.419507    7192 certs.go:484] found cert: C:\Users\TAAFAAL4\.minikube\certs\key.pem (1675 bytes)
I0219 09:16:28.420547    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0219 09:16:28.440395    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0219 09:16:28.458656    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0219 09:16:28.479460    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0219 09:16:28.499245    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0219 09:16:28.516494    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0219 09:16:28.534283    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0219 09:16:28.554298    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0219 09:16:28.573553    7192 ssh_runner.go:362] scp C:\Users\TAAFAAL4\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0219 09:16:28.608430    7192 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0219 09:16:28.639372    7192 ssh_runner.go:195] Run: openssl version
I0219 09:16:28.717251    7192 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0219 09:16:28.744016    7192 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0219 09:16:28.748918    7192 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 18 13:21 /usr/share/ca-certificates/minikubeCA.pem
I0219 09:16:28.756656    7192 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0219 09:16:28.789549    7192 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0219 09:16:28.820321    7192 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0219 09:16:28.837306    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0219 09:16:28.853953    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0219 09:16:28.872977    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0219 09:16:28.919288    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0219 09:16:28.933895    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0219 09:16:28.948073    7192 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0219 09:16:28.953978    7192 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\TAAFAAL4:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0219 09:16:28.960192    7192 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0219 09:16:28.988833    7192 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0219 09:16:28.997004    7192 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0219 09:16:28.997004    7192 kubeadm.go:593] restartPrimaryControlPlane start ...
I0219 09:16:29.007478    7192 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0219 09:16:29.015889    7192 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0219 09:16:29.022143    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.104523    7192 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:56045"
I0219 09:16:29.104523    7192 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:56045, want: 127.0.0.1:60109
I0219 09:16:29.105773    7192 kubeconfig.go:62] C:\Users\TAAFAAL4\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0219 09:16:29.106508    7192 lock.go:35] WriteFile acquiring C:\Users\TAAFAAL4\.kube\config: {Name:mkd2abdaaacabdfc2383282f3e09e9977eff3bf4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 09:16:29.141294    7192 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0219 09:16:29.152268    7192 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0219 09:16:29.152268    7192 kubeadm.go:597] duration metric: took 155.2641ms to restartPrimaryControlPlane
I0219 09:16:29.152268    7192 kubeadm.go:394] duration metric: took 198.2898ms to StartCluster
I0219 09:16:29.152268    7192 settings.go:142] acquiring lock: {Name:mk81f64253828b0c323ff80c3b652c7c7ddff4df Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 09:16:29.152792    7192 settings.go:150] Updating kubeconfig:  C:\Users\TAAFAAL4\.kube\config
I0219 09:16:29.153858    7192 lock.go:35] WriteFile acquiring C:\Users\TAAFAAL4\.kube\config: {Name:mkd2abdaaacabdfc2383282f3e09e9977eff3bf4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 09:16:29.154373    7192 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0219 09:16:29.154373    7192 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0219 09:16:29.154373    7192 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0219 09:16:29.154373    7192 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0219 09:16:29.154373    7192 addons.go:247] addon storage-provisioner should already be in state true
I0219 09:16:29.154899    7192 host.go:66] Checking if "minikube" exists ...
I0219 09:16:29.154899    7192 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0219 09:16:29.154899    7192 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0219 09:16:29.154899    7192 addons.go:69] Setting ingress=true in profile "minikube"
I0219 09:16:29.154899    7192 addons.go:238] Setting addon ingress=true in "minikube"
W0219 09:16:29.154899    7192 addons.go:247] addon ingress should already be in state true
I0219 09:16:29.154899    7192 addons.go:69] Setting dashboard=true in profile "minikube"
I0219 09:16:29.154899    7192 addons.go:238] Setting addon dashboard=true in "minikube"
W0219 09:16:29.154899    7192 addons.go:247] addon dashboard should already be in state true
I0219 09:16:29.154899    7192 host.go:66] Checking if "minikube" exists ...
I0219 09:16:29.154899    7192 addons.go:69] Setting metrics-server=true in profile "minikube"
I0219 09:16:29.154899    7192 addons.go:238] Setting addon metrics-server=true in "minikube"
W0219 09:16:29.154899    7192 addons.go:247] addon metrics-server should already be in state true
I0219 09:16:29.155487    7192 host.go:66] Checking if "minikube" exists ...
I0219 09:16:29.155487    7192 host.go:66] Checking if "minikube" exists ...
I0219 09:16:29.155487    7192 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0219 09:16:29.162962    7192 out.go:177] 🔎  Verifiziere Kubernetes Komponenten...
I0219 09:16:29.180607    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.181761    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.183560    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.184064    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.187215    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.193487    7192 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 09:16:29.262841    7192 out.go:177]     ▪ Verwende Image gcr.io/k8s-minikube/storage-provisioner:v5
I0219 09:16:29.267572    7192 out.go:177]     ▪ Verwende Image docker.io/kubernetesui/dashboard:v2.7.0
I0219 09:16:29.264406    7192 out.go:177]     ▪ Verwende Image registry.k8s.io/metrics-server/metrics-server:v0.7.2
I0219 09:16:29.270729    7192 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0219 09:16:29.270729    7192 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0219 09:16:29.273921    7192 addons.go:435] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0219 09:16:29.274441    7192 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0219 09:16:29.277068    7192 out.go:177]     ▪ Verwende Image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0219 09:16:29.279702    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0219 09:16:29.279702    7192 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0219 09:16:29.281806    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.284409    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.292254    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.292254    7192 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0219 09:16:29.292254    7192 addons.go:247] addon default-storageclass should already be in state true
I0219 09:16:29.292786    7192 host.go:66] Checking if "minikube" exists ...
I0219 09:16:29.315818    7192 out.go:177] 💡  Nachdem das Addon aktiviert wurde, führen Sie bitte "minikube tunnel" aus, dann sind ihre Resourcen über "127.0.0.1" erreichbar
I0219 09:16:29.317937    7192 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 09:16:29.323037    7192 out.go:177]     ▪ Verwende Image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
I0219 09:16:29.325781    7192 out.go:177]     ▪ Verwende Image registry.k8s.io/ingress-nginx/controller:v1.11.3
I0219 09:16:29.330726    7192 out.go:177]     ▪ Verwende Image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
I0219 09:16:29.333975    7192 addons.go:435] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0219 09:16:29.333975    7192 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I0219 09:16:29.346435    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.384830    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:29.392051    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:29.393613    7192 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0219 09:16:29.394159    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:29.397859    7192 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0219 09:16:29.397859    7192 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0219 09:16:29.405284    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.415587    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:29.469344    7192 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60104 SSHKeyPath:C:\Users\TAAFAAL4\.minikube\machines\minikube\id_rsa Username:docker}
I0219 09:16:29.541204    7192 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 09:16:29.613661    7192 api_server.go:52] waiting for apiserver process to appear ...
I0219 09:16:29.626462    7192 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 09:16:29.652959    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0219 09:16:29.652959    7192 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0219 09:16:29.667293    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0219 09:16:29.740787    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0219 09:16:29.740787    7192 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0219 09:16:29.752394    7192 addons.go:435] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0219 09:16:29.752394    7192 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0219 09:16:29.761350    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0219 09:16:29.761350    7192 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0219 09:16:29.773906    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0219 09:16:29.774435    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0219 09:16:29.838622    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0219 09:16:29.838622    7192 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0219 09:16:29.843502    7192 addons.go:435] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0219 09:16:29.843502    7192 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0219 09:16:29.859106    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0219 09:16:29.859106    7192 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0219 09:16:29.863331    7192 addons.go:435] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0219 09:16:29.863331    7192 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0219 09:16:29.882160    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0219 09:16:29.882160    7192 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0219 09:16:29.897179    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0219 09:16:29.951358    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0219 09:16:29.951358    7192 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0219 09:16:29.970735    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0219 09:16:29.970735    7192 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0219 09:16:29.989818    7192 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0219 09:16:29.989818    7192 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0219 09:16:30.078537    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0219 09:16:30.128073    7192 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 09:16:30.388516    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.388516    7192 retry.go:31] will retry after 237.075883ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0219 09:16:30.463935    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.464445    7192 retry.go:31] will retry after 341.34479ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0219 09:16:30.464445    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.464496    7192 retry.go:31] will retry after 338.287751ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0219 09:16:30.465437    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.465437    7192 retry.go:31] will retry after 186.406615ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0219 09:16:30.465437    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.465437    7192 retry.go:31] will retry after 359.190027ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.633047    7192 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 09:16:30.644804    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0219 09:16:30.664099    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0219 09:16:30.667013    7192 api_server.go:72] duration metric: took 1.5126403s to wait for apiserver process to appear ...
I0219 09:16:30.667128    7192 api_server.go:88] waiting for apiserver healthz status ...
I0219 09:16:30.667128    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:30.670064    7192 api_server.go:269] stopped: https://127.0.0.1:60109/healthz: Get "https://127.0.0.1:60109/healthz": EOF
W0219 09:16:30.782229    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.782229    7192 retry.go:31] will retry after 266.876377ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0219 09:16:30.782229    7192 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.782229    7192 retry.go:31] will retry after 294.208781ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/metrics-apiservice.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-rbac.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/metrics-server-service.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0219 09:16:30.818121    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0219 09:16:30.822311    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0219 09:16:30.837679    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0219 09:16:31.058483    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0219 09:16:31.088354    7192 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0219 09:16:31.167584    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:33.193647    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0219 09:16:33.193647    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0219 09:16:33.193647    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:33.398884    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:33.398884    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:33.599877    7192 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.7817566s)
I0219 09:16:33.667608    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:33.703142    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:33.703142    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:34.167618    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:34.232677    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:34.232677    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:34.667439    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:34.753994    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:34.753994    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:35.168724    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:35.179764    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:35.179764    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:35.667567    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:35.702327    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:35.702327    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:36.167759    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:36.231197    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:36.231197    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:36.667919    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:36.743204    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:36.743204    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:37.168936    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:37.177003    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:37.177003    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:37.667465    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:37.700741    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:37.700741    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:38.167284    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:38.220525    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0219 09:16:38.220525    7192 api_server.go:103] status: https://127.0.0.1:60109/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0219 09:16:38.530451    7192 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (7.70814s)
I0219 09:16:38.530451    7192 addons.go:479] Verifying addon ingress=true in "minikube"
I0219 09:16:38.530451    7192 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (7.6927716s)
I0219 09:16:38.530970    7192 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (7.442616s)
I0219 09:16:38.530970    7192 addons.go:479] Verifying addon metrics-server=true in "minikube"
I0219 09:16:38.530970    7192 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.472487s)
I0219 09:16:38.532051    7192 out.go:177] 💡  Einige Dashboard Features erfordern das metrics-server addon. Um alle Features zu aktivieren:

	minikube addons enable metrics-server

I0219 09:16:38.533091    7192 out.go:177] 🔎  Verifiziere ingress Addon...
I0219 09:16:38.537920    7192 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0219 09:16:38.546011    7192 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0219 09:16:38.546011    7192 kapi.go:107] duration metric: took 8.0907ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0219 09:16:38.547063    7192 out.go:177] 🌟  Addons aktiviert: default-storageclass, metrics-server, storage-provisioner, dashboard, ingress
I0219 09:16:38.548160    7192 addons.go:514] duration metric: took 9.3937866s for enable addons: enabled=[default-storageclass metrics-server storage-provisioner dashboard ingress]
I0219 09:16:38.667470    7192 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60109/healthz ...
I0219 09:16:38.674079    7192 api_server.go:279] https://127.0.0.1:60109/healthz returned 200:
ok
I0219 09:16:38.677303    7192 api_server.go:141] control plane version: v1.32.0
I0219 09:16:38.677303    7192 api_server.go:131] duration metric: took 8.0101752s to wait for apiserver health ...
I0219 09:16:38.677303    7192 system_pods.go:43] waiting for kube-system pods to appear ...
I0219 09:16:38.683115    7192 system_pods.go:59] 8 kube-system pods found
I0219 09:16:38.683115    7192 system_pods.go:61] "coredns-668d6bf9bc-29hqv" [72dffdca-ba25-4ab9-ad9f-9948610744c0] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0219 09:16:38.683115    7192 system_pods.go:61] "etcd-minikube" [4b5f193c-5060-477f-9bd1-0f4cb0b1d398] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0219 09:16:38.683115    7192 system_pods.go:61] "kube-apiserver-minikube" [7816471f-d751-4ecc-8d4b-03812987de34] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0219 09:16:38.683115    7192 system_pods.go:61] "kube-controller-manager-minikube" [bfae607d-8416-4941-9309-854554548855] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0219 09:16:38.683115    7192 system_pods.go:61] "kube-proxy-vzb5c" [407cab90-f0ff-462c-bb6a-ccce63fa41b7] Running
I0219 09:16:38.683115    7192 system_pods.go:61] "kube-scheduler-minikube" [da4eddf0-beb6-4788-ac50-42e974bda559] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0219 09:16:38.683115    7192 system_pods.go:61] "metrics-server-7fbb699795-4nq5q" [db579db6-4d96-44ff-92ef-e1773da6dc3a] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0219 09:16:38.683115    7192 system_pods.go:61] "storage-provisioner" [1e216870-111d-451e-8bcf-5ffcfa281da6] Running
I0219 09:16:38.683115    7192 system_pods.go:74] duration metric: took 5.8121ms to wait for pod list to return data ...
I0219 09:16:38.683115    7192 kubeadm.go:582] duration metric: took 9.5287418s to wait for: map[apiserver:true system_pods:true]
I0219 09:16:38.683115    7192 node_conditions.go:102] verifying NodePressure condition ...
I0219 09:16:38.687329    7192 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0219 09:16:38.687329    7192 node_conditions.go:123] node cpu capacity is 8
I0219 09:16:38.687329    7192 node_conditions.go:105] duration metric: took 4.2139ms to run NodePressure ...
I0219 09:16:38.687329    7192 start.go:241] waiting for startup goroutines ...
I0219 09:16:38.687329    7192 start.go:246] waiting for cluster config update ...
I0219 09:16:38.687329    7192 start.go:255] writing updated cluster config ...
I0219 09:16:38.702083    7192 ssh_runner.go:195] Run: rm -f paused
I0219 09:16:39.202409    7192 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0219 09:16:39.204749    7192 out.go:177] 🏄  Fertig! kubectl ist jetzt für die standardmäßige (default) Verwendung des Clusters "minikube" und des Namespaces "default" konfiguriert


==> Docker <==
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-56d7c84fd4-bjvjj_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"37d6135e91d995fe57163ec968b71610ba824cf90a41448f67edc634e63c823a\""
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-6c7bw_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1fa1e972e008e5aa906622d6a753a4743021e0922d6cf0fd7c7fc399949a917d\""
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-29hqv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4350ee154de178ddd0173385717b76aaaeb836ea6f44421757b6549e8a61f298\""
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-29hqv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6cb1a00fed802b613090f2a0b8f19ddfc1669ef87fd78ee9b681b494c9ec3cdd\""
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"153827f159c3f2e3fac309f34bb230ff1cb4536b148afcf3bf8d827273877ad7\". Proceed without further sandbox information."
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"58a766ed0312e9c21b6fbb9a2de382d1c8084f91246ef84277e03ed8bd4288a0\". Proceed without further sandbox information."
Feb 19 08:16:29 minikube cri-dockerd[1566]: time="2025-02-19T08:16:29Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"873cd62151c240cc57b36abcabb595d4e1ee46a87b855c0036c1a9491fc04d86\". Proceed without further sandbox information."
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"b599c12646bf79d47c6b454eb4aebeb276c9d0b48368a137c646567e5a91c4a1\". Proceed without further sandbox information."
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa77857f3c529611e8abd300508581a0e1a082b597c58535007bf1efc5ceae49/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8fd94a65b06a3e8a571675b024da614bd813ae5f7e91ca2bb8b7c8a5b0e4b4c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e70cb7eabaf1d52d4325f266889e6fc269141859c52d76ae5b9f657fcffa82f9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-krrth_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"534058fb157332ad5c007730528329cff4d8544a99df59cae66fd2f8e92202b4\""
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea6c3e2e984881b468bcf6a4b2bcbcad28c0509cef4edf7260dad725181f49f0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:30 minikube cri-dockerd[1566]: time="2025-02-19T08:16:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-5sqlr_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"95082575c894b17fa99197c5bb6be7880063a74b93f6f901a81af4f05fd97046\""
Feb 19 08:16:31 minikube cri-dockerd[1566]: time="2025-02-19T08:16:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-7fbb699795-4nq5q_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c46143d16754a7585ea55cc78958190293252de52e3b81d5596a3470c225e0d7\""
Feb 19 08:16:31 minikube cri-dockerd[1566]: time="2025-02-19T08:16:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-56d7c84fd4-bjvjj_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f854c8bbd52f49292e60fcf0fb4773e68d250698e649dafd998f2d85283fefa9\""
Feb 19 08:16:31 minikube cri-dockerd[1566]: time="2025-02-19T08:16:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-29hqv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4350ee154de178ddd0173385717b76aaaeb836ea6f44421757b6549e8a61f298\""
Feb 19 08:16:31 minikube cri-dockerd[1566]: time="2025-02-19T08:16:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-29hqv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6cb1a00fed802b613090f2a0b8f19ddfc1669ef87fd78ee9b681b494c9ec3cdd\""
Feb 19 08:16:34 minikube cri-dockerd[1566]: time="2025-02-19T08:16:34Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 19 08:16:35 minikube cri-dockerd[1566]: time="2025-02-19T08:16:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0de25243bf957b8199bbf01ded560e37a208da798863fbab7204d5c96e891cb6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:36 minikube cri-dockerd[1566]: time="2025-02-19T08:16:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/786471f791df5aa9d02bf1665980194c98997b682f1f98b768724de8c1f7e64f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:16:36 minikube cri-dockerd[1566]: time="2025-02-19T08:16:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1f8a2a1a9633af38744c6101d03144be79e5f2a906392a48a1a09f425668adc/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:16:36 minikube cri-dockerd[1566]: time="2025-02-19T08:16:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d8295d65cfb559b041a13f8b7372f668e343e348d1a365cc0e9417eb44b55c71/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:16:36 minikube cri-dockerd[1566]: time="2025-02-19T08:16:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d870465dae1b4ad7a8d99ff42333c94a993045e76e3c1e70de7c26fa36735f4/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:16:36 minikube cri-dockerd[1566]: time="2025-02-19T08:16:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44d51c7bbdb70a6411914098829f144bf90f7134437ea45fcdf501b9b400a43c/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:16:37 minikube cri-dockerd[1566]: time="2025-02-19T08:16:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2206e661b29fa85e93201f252be6db097c280428441b77c5c469b79dc1895e77/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 19 08:17:08 minikube dockerd[1237]: time="2025-02-19T08:17:08.934278858Z" level=info msg="ignoring event" container=37f596c04238a55c032c2ebc52ff9031dc536c35a26cb09507285679cae01bc2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 08:17:09 minikube dockerd[1237]: time="2025-02-19T08:17:09.664449583Z" level=info msg="ignoring event" container=7ce8a461c453febabf9754a2e35f7039ae6aa87c4c9886e40c9661ba509734d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 08:18:53 minikube cri-dockerd[1566]: time="2025-02-19T08:18:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3feafa8c5a19f7234d25c1061f5d502bd36fc48f1a73f969e78c3ce8db9acfcb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:18:59 minikube cri-dockerd[1566]: time="2025-02-19T08:18:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d96d2c117198db14b426f904212e722302cfebf8413310938a61b46c7ccff46d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 19 08:19:10 minikube dockerd[1237]: time="2025-02-19T08:19:10.292620865Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:19:10 minikube dockerd[1237]: time="2025-02-19T08:19:10.292661107Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:19:10 minikube dockerd[1237]: time="2025-02-19T08:19:10.295957871Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:19:25 minikube dockerd[1237]: time="2025-02-19T08:19:25.309292853Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:25 minikube dockerd[1237]: time="2025-02-19T08:19:25.309337788Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:25 minikube dockerd[1237]: time="2025-02-19T08:19:25.316887947Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:41 minikube dockerd[1237]: time="2025-02-19T08:19:41.873484462Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:41 minikube dockerd[1237]: time="2025-02-19T08:19:41.873544229Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:41 minikube dockerd[1237]: time="2025-02-19T08:19:41.877993365Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:19:56 minikube dockerd[1237]: time="2025-02-19T08:19:56.992916604Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:19:56 minikube dockerd[1237]: time="2025-02-19T08:19:56.992957031Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:19:57 minikube dockerd[1237]: time="2025-02-19T08:19:57.000084051Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout"
Feb 19 08:20:22 minikube dockerd[1237]: time="2025-02-19T08:20:22.360365710Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Feb 19 08:20:22 minikube dockerd[1237]: time="2025-02-19T08:20:22.360410196Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Feb 19 08:20:22 minikube dockerd[1237]: time="2025-02-19T08:20:22.364170805Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Feb 19 08:20:38 minikube dockerd[1237]: time="2025-02-19T08:20:38.780818627Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:20:38 minikube dockerd[1237]: time="2025-02-19T08:20:38.780867059Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:20:38 minikube dockerd[1237]: time="2025-02-19T08:20:38.784184578Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:28 minikube dockerd[1237]: time="2025-02-19T08:21:28.221096420Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:28 minikube dockerd[1237]: time="2025-02-19T08:21:28.221154111Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:28 minikube dockerd[1237]: time="2025-02-19T08:21:28.225282249Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:45 minikube dockerd[1237]: time="2025-02-19T08:21:45.888844964Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:45 minikube dockerd[1237]: time="2025-02-19T08:21:45.888895085Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:21:45 minikube dockerd[1237]: time="2025-02-19T08:21:45.892326491Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:23:19 minikube dockerd[1237]: time="2025-02-19T08:23:19.671508783Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:23:19 minikube dockerd[1237]: time="2025-02-19T08:23:19.671571618Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:23:19 minikube dockerd[1237]: time="2025-02-19T08:23:19.678022526Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 08:23:39 minikube dockerd[1237]: time="2025-02-19T08:23:39.663773527Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Feb 19 08:23:39 minikube dockerd[1237]: time="2025-02-19T08:23:39.663871236Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Feb 19 08:23:39 minikube dockerd[1237]: time="2025-02-19T08:23:39.677740861Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
b9797d26291fb       07655ddf2eebe                                                                                                                7 minutes ago       Running             kubernetes-dashboard        4                   a1f8a2a1a9633       kubernetes-dashboard-7779f9b69b-krrth
3cfdb312087cf       6e38f40d628db                                                                                                                7 minutes ago       Running             storage-provisioner         4                   786471f791df5       storage-provisioner
a8823d79acaf7       ee44bc2368033                                                                                                                7 minutes ago       Running             controller                  2                   44d51c7bbdb70       ingress-nginx-controller-56d7c84fd4-bjvjj
8af9500cd7196       c69fa2e9cbf5f                                                                                                                7 minutes ago       Running             coredns                     2                   2206e661b29fa       coredns-668d6bf9bc-29hqv
a2a5c6db37f1c       48d9cfaaf3904                                                                                                                7 minutes ago       Running             metrics-server              2                   0d870465dae1b       metrics-server-7fbb699795-4nq5q
7ce8a461c453f       07655ddf2eebe                                                                                                                7 minutes ago       Exited              kubernetes-dashboard        3                   a1f8a2a1a9633       kubernetes-dashboard-7779f9b69b-krrth
cd2b33c7d4c11       115053965e86b                                                                                                                7 minutes ago       Running             dashboard-metrics-scraper   2                   d8295d65cfb55       dashboard-metrics-scraper-5d59dccf9b-5sqlr
37f596c04238a       6e38f40d628db                                                                                                                7 minutes ago       Exited              storage-provisioner         3                   786471f791df5       storage-provisioner
73939e8abd311       040f9f8aac8cd                                                                                                                7 minutes ago       Running             kube-proxy                  2                   0de25243bf957       kube-proxy-vzb5c
8809afb66cc98       8cab3d2a8bd0f                                                                                                                8 minutes ago       Running             kube-controller-manager     2                   ea6c3e2e98488       kube-controller-manager-minikube
1ddd92aef2c14       a389e107f4ff1                                                                                                                8 minutes ago       Running             kube-scheduler              2                   aa77857f3c529       kube-scheduler-minikube
63ed5b9aefab1       c2e17b8d0f4a3                                                                                                                8 minutes ago       Running             kube-apiserver              2                   e70cb7eabaf1d       kube-apiserver-minikube
75fe39140d4c7       a9e7e6b294baf                                                                                                                8 minutes ago       Running             etcd                        2                   c8fd94a65b06a       etcd-minikube
f374be1d31b2a       ee44bc2368033                                                                                                                19 hours ago        Exited              controller                  1                   f854c8bbd52f4       ingress-nginx-controller-56d7c84fd4-bjvjj
14e2775d99eef       115053965e86b                                                                                                                19 hours ago        Exited              dashboard-metrics-scraper   1                   95082575c894b       dashboard-metrics-scraper-5d59dccf9b-5sqlr
0b885c17db54a       a9e7e6b294baf                                                                                                                19 hours ago        Exited              etcd                        1                   d1378565af410       etcd-minikube
e542b84952749       a389e107f4ff1                                                                                                                19 hours ago        Exited              kube-scheduler              1                   6e4dd8a462281       kube-scheduler-minikube
b2095ca1514d5       48d9cfaaf3904                                                                                                                19 hours ago        Exited              metrics-server              1                   c46143d16754a       metrics-server-7fbb699795-4nq5q
343d15a79827a       c69fa2e9cbf5f                                                                                                                19 hours ago        Exited              coredns                     1                   4350ee154de17       coredns-668d6bf9bc-29hqv
1823965137bb3       c2e17b8d0f4a3                                                                                                                19 hours ago        Exited              kube-apiserver              1                   52e545fe23bc6       kube-apiserver-minikube
5d5f75713c918       040f9f8aac8cd                                                                                                                19 hours ago        Exited              kube-proxy                  1                   d191fd5207ffa       kube-proxy-vzb5c
b0687f5ab3662       8cab3d2a8bd0f                                                                                                                19 hours ago        Exited              kube-controller-manager     1                   7053b55be0f2d       kube-controller-manager-minikube
a1ff9b8a2b4d5       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   19 hours ago        Exited              patch                       0                   1fa1e972e008e       ingress-nginx-admission-patch-6c7bw
dddc0e4705529       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   19 hours ago        Exited              create                      0                   0047f2bd9e2d0       ingress-nginx-admission-create-7j7n8


==> controller_ingress [a8823d79acaf] <==
W0219 08:16:38.742550       6 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0219 08:16:38.742691       6 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0219 08:17:11.176045       6 main.go:245] Initial connection to the Kubernetes API server was retried 1 times.
I0219 08:17:11.176070       6 main.go:248] "Running in Kubernetes cluster" major="1" minor="32" git="v1.32.0" state="clean" commit="70d3cc986aa8221cd1dfb1121852688902d3bf53" platform="linux/amd64"
I0219 08:17:11.326269       6 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0219 08:17:11.344371       6 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0219 08:17:11.352480       6 nginx.go:271] "Starting NGINX Ingress controller"
I0219 08:17:11.356472       6 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"a34639b2-3962-4948-8c0f-91417622057d", APIVersion:"v1", ResourceVersion:"456", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0219 08:17:11.359451       6 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"abf79493-8170-4b6f-a117-790f5b8d39d6", APIVersion:"v1", ResourceVersion:"457", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0219 08:17:11.359489       6 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"9f275828-8c67-4149-afd3-7222afdc7a3d", APIVersion:"v1", ResourceVersion:"458", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0219 08:17:12.554654       6 nginx.go:317] "Starting NGINX process"
I0219 08:17:12.554712       6 leaderelection.go:254] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0219 08:17:12.554993       6 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0219 08:17:12.555201       6 controller.go:193] "Configuration changes detected, backend reload required"
I0219 08:17:12.563379       6 leaderelection.go:268] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0219 08:17:12.563451       6 status.go:85] "New leader elected" identity="ingress-nginx-controller-56d7c84fd4-bjvjj"
I0219 08:17:12.565917       6 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-bjvjj" node="minikube"
I0219 08:17:12.588057       6 controller.go:213] "Backend successfully reloaded"
I0219 08:17:12.588155       6 controller.go:224] "Initial sync, sleeping for 1 second"
I0219 08:17:12.588227       6 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56d7c84fd4-bjvjj", UID:"a762f705-5593-47d5-a8f4-5bb73903ad8c", APIVersion:"v1", ResourceVersion:"7734", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.11.3
  Build:         0106de65cfccb74405a6dfa7d9daffc6f0a6ef1a
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------



==> controller_ingress [f374be1d31b2] <==
W0218 13:25:40.729766       8 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0218 13:25:40.729857       8 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0218 13:25:42.393902       8 main.go:248] "Running in Kubernetes cluster" major="1" minor="32" git="v1.32.0" state="clean" commit="70d3cc986aa8221cd1dfb1121852688902d3bf53" platform="linux/amd64"
I0218 13:25:43.022940       8 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0218 13:25:43.200867       8 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0218 13:25:43.218796       8 nginx.go:271] "Starting NGINX Ingress controller"
I0218 13:25:43.302655       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"a34639b2-3962-4948-8c0f-91417622057d", APIVersion:"v1", ResourceVersion:"456", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0218 13:25:43.303818       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"abf79493-8170-4b6f-a117-790f5b8d39d6", APIVersion:"v1", ResourceVersion:"457", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0218 13:25:43.303855       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"9f275828-8c67-4149-afd3-7222afdc7a3d", APIVersion:"v1", ResourceVersion:"458", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0218 13:25:47.227196       8 nginx.go:317] "Starting NGINX process"
I0218 13:25:47.227259       8 leaderelection.go:254] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0218 13:25:47.227851       8 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0218 13:25:47.228296       8 controller.go:193] "Configuration changes detected, backend reload required"
I0218 13:25:47.253021       8 leaderelection.go:268] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0218 13:25:47.253224       8 status.go:85] "New leader elected" identity="ingress-nginx-controller-56d7c84fd4-bjvjj"
I0218 13:25:47.330130       8 controller.go:213] "Backend successfully reloaded"
I0218 13:25:47.330221       8 controller.go:224] "Initial sync, sleeping for 1 second"
I0218 13:25:47.330256       8 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56d7c84fd4-bjvjj", UID:"a762f705-5593-47d5-a8f4-5bb73903ad8c", APIVersion:"v1", ResourceVersion:"575", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0218 13:25:47.333546       8 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-bjvjj" node="minikube"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.11.3
  Build:         0106de65cfccb74405a6dfa7d9daffc6f0a6ef1a
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------



==> coredns [343d15a79827] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:45745 - 23417 "HINFO IN 6280581733179241677.8566751304496502371. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.092742172s


==> coredns [8af9500cd719] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:35771 - 61272 "HINFO IN 547436993221934388.869419167719031536. udp 55 false 512" NXDOMAIN qr,rd,ra 55 0.08526393s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1004404661]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Feb-2025 08:16:38.807) (total time: 30003ms):
Trace[1004404661]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (08:17:10.113)
Trace[1004404661]: [30.003648188s] [30.003648188s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[905000064]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Feb-2025 08:16:38.807) (total time: 30003ms):
Trace[905000064]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (08:17:10.113)
Trace[905000064]: [30.00378323s] [30.00378323s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[29148279]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (19-Feb-2025 08:16:38.807) (total time: 30003ms):
Trace[29148279]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (08:17:10.113)
Trace[29148279]: [30.003728264s] [30.003728264s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_18T14_21_19_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Feb 2025 13:21:15 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 19 Feb 2025 08:24:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 19 Feb 2025 08:19:37 +0000   Tue, 18 Feb 2025 13:21:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 19 Feb 2025 08:19:37 +0000   Tue, 18 Feb 2025 13:21:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 19 Feb 2025 08:19:37 +0000   Tue, 18 Feb 2025 13:21:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 19 Feb 2025 08:19:37 +0000   Tue, 18 Feb 2025 13:21:15 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16230416Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16230416Ki
  pods:               110
System Info:
  Machine ID:                 ba36578f6de14cbca9a6cb8920002361
  System UUID:                ba36578f6de14cbca9a6cb8920002361
  Boot ID:                    bba17c50-c41c-4c31-9760-7baf622742b9
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     mongo-deployment-7cccf8b6d8-x4xbh             0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m41s
  default                     webapp-deployment-6bb4795f54-fzpvx            0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m35s
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-bjvjj     100m (1%)     0 (0%)      90Mi (0%)        0 (0%)         19h
  kube-system                 coredns-668d6bf9bc-29hqv                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     19h
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         19h
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-proxy-vzb5c                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 metrics-server-7fbb699795-4nq5q               100m (1%)     0 (0%)      200Mi (1%)       0 (0%)         19h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-5sqlr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-krrth         0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (11%)  0 (0%)
  memory             460Mi (2%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           7m54s                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  8m4s                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m4s                 kubelet          Starting kubelet.
  Warning  CgroupV1                           8m4s                 kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            8m4s (x8 over 8m4s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m4s (x8 over 8m4s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m4s (x7 over 8m4s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            8m4s                 kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           7m59s                kubelet          Node minikube has been rebooted, boot id: bba17c50-c41c-4c31-9760-7baf622742b9
  Normal   RegisteredNode                     7m52s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Feb19 08:10] PCI: Fatal: No config space access function found
[  +0.037013] PCI: System does not support PCI
[  +0.611263] kvm: already loaded the other module
[  +3.080898] FS-Cache: Duplicate cookie detected
[  +0.000727] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.002022] FS-Cache: O-cookie d=000000007f88d37a{9P.session} n=0000000000f69cf5
[  +0.005181] FS-Cache: O-key=[10] '34323934393337363930'
[  +0.001004] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.002389] FS-Cache: N-cookie d=000000007f88d37a{9P.session} n=00000000d41dd5a3
[  +0.001635] FS-Cache: N-key=[10] '34323934393337363930'
[  +0.096948] FS-Cache: Duplicate cookie detected
[  +0.001374] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.001149] FS-Cache: O-cookie d=000000007f88d37a{9P.session} n=000000007930b253
[  +0.000426] FS-Cache: O-key=[10] '34323934393337373031'
[  +0.000269] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000370] FS-Cache: N-cookie d=000000007f88d37a{9P.session} n=00000000412b5308
[  +0.000466] FS-Cache: N-key=[10] '34323934393337373031'
[  +0.521678] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#319 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +1.224884] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.020206] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Zurich not found. Is the tzdata package installed?
[  +0.506771] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.035159] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.055443] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.017129] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.032153] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.344795] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.039002] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.021767] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.039028] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.087458] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.117030] Failed to connect to bus: No such file or directory
[  +0.256501] Failed to connect to bus: No such file or directory
[  +0.256167] Failed to connect to bus: No such file or directory
[  +0.760913] systemd-journald[68]: File /var/log/journal/18e46e51307e4928bc061a0534182de8/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.494718] WSL (150) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +25.006730] new mount options do not match the existing superblock, will be ignored
[  +0.001285] netlink: 'init': attribute type 4 has an invalid length.
[Feb19 08:17] tmpfs: Unknown parameter 'noswap'


==> etcd [0b885c17db54] <==
{"level":"info","ts":"2025-02-18T13:53:30.848738Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2135,"took":"3.897932ms","hash":848497378,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1851392,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-18T13:53:30.848827Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":848497378,"revision":2135,"compact-revision":1856}
{"level":"info","ts":"2025-02-18T13:58:58.143085Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2414}
{"level":"info","ts":"2025-02-18T13:58:58.145963Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2414,"took":"2.675657ms","hash":3958488366,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1843200,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T13:58:58.145996Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3958488366,"revision":2414,"compact-revision":2135}
{"level":"info","ts":"2025-02-18T14:04:25.426357Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2696}
{"level":"info","ts":"2025-02-18T14:04:25.429339Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2696,"took":"2.776105ms","hash":2316950949,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1835008,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:04:25.429372Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2316950949,"revision":2696,"compact-revision":2414}
{"level":"info","ts":"2025-02-18T14:09:52.559317Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2975}
{"level":"info","ts":"2025-02-18T14:09:52.563329Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2975,"took":"3.786002ms","hash":3902189247,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1835008,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:09:52.563377Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3902189247,"revision":2975,"compact-revision":2696}
{"level":"info","ts":"2025-02-18T14:15:19.609441Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3255}
{"level":"info","ts":"2025-02-18T14:15:19.612516Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3255,"took":"2.885129ms","hash":1674370988,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:15:19.612559Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1674370988,"revision":3255,"compact-revision":2975}
{"level":"info","ts":"2025-02-18T14:20:46.893470Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3535}
{"level":"info","ts":"2025-02-18T14:20:46.896931Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3535,"took":"3.226889ms","hash":980611058,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1818624,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:20:46.896976Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":980611058,"revision":3535,"compact-revision":3255}
{"level":"info","ts":"2025-02-18T14:26:14.175991Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3814}
{"level":"info","ts":"2025-02-18T14:26:14.178960Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3814,"took":"2.711049ms","hash":816691365,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1830912,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:26:14.178997Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":816691365,"revision":3814,"compact-revision":3535}
{"level":"info","ts":"2025-02-18T14:31:41.471336Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4094}
{"level":"info","ts":"2025-02-18T14:31:41.474679Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4094,"took":"3.0989ms","hash":470637612,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-18T14:31:41.474740Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":470637612,"revision":4094,"compact-revision":3814}
{"level":"info","ts":"2025-02-18T14:37:08.763689Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4374}
{"level":"info","ts":"2025-02-18T14:37:08.768514Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4374,"took":"4.300823ms","hash":1583307618,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-18T14:37:08.768577Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1583307618,"revision":4374,"compact-revision":4094}
{"level":"info","ts":"2025-02-18T14:42:36.087666Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4660}
{"level":"info","ts":"2025-02-18T14:42:36.092292Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4660,"took":"4.343366ms","hash":1882571721,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":2170880,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-02-18T14:42:36.092335Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1882571721,"revision":4660,"compact-revision":4374}
{"level":"info","ts":"2025-02-18T14:48:03.351128Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4941}
{"level":"info","ts":"2025-02-18T14:48:03.354882Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4941,"took":"3.572825ms","hash":210428177,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T14:48:03.354930Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":210428177,"revision":4941,"compact-revision":4660}
{"level":"info","ts":"2025-02-18T14:53:30.643239Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5220}
{"level":"info","ts":"2025-02-18T14:53:30.652886Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5220,"took":"9.210698ms","hash":3055700739,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1675264,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T14:53:30.653034Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3055700739,"revision":5220,"compact-revision":4941}
{"level":"info","ts":"2025-02-18T14:58:57.935860Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5500}
{"level":"info","ts":"2025-02-18T14:58:57.943080Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5500,"took":"6.849144ms","hash":1093674377,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1695744,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T14:58:57.943227Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1093674377,"revision":5500,"compact-revision":5220}
{"level":"info","ts":"2025-02-18T15:04:25.194655Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5780}
{"level":"info","ts":"2025-02-18T15:04:25.203062Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5780,"took":"8.056486ms","hash":1377835059,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1683456,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:04:25.203164Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1377835059,"revision":5780,"compact-revision":5500}
{"level":"info","ts":"2025-02-18T15:09:46.944367Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6059}
{"level":"info","ts":"2025-02-18T15:09:46.952835Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6059,"took":"7.941686ms","hash":2165889547,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1675264,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:09:46.952965Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2165889547,"revision":6059,"compact-revision":5780}
{"level":"info","ts":"2025-02-18T15:15:13.694614Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6339}
{"level":"info","ts":"2025-02-18T15:15:13.701989Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6339,"took":"7.008917ms","hash":451099291,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:15:13.702109Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":451099291,"revision":6339,"compact-revision":6059}
{"level":"info","ts":"2025-02-18T15:20:41.006691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6617}
{"level":"info","ts":"2025-02-18T15:20:41.012825Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6617,"took":"5.742418ms","hash":97114081,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:20:41.012927Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":97114081,"revision":6617,"compact-revision":6339}
{"level":"info","ts":"2025-02-18T15:26:09.665950Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6897}
{"level":"info","ts":"2025-02-18T15:26:09.669997Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6897,"took":"3.86082ms","hash":3919346212,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1708032,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:26:09.670061Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3919346212,"revision":6897,"compact-revision":6617}
{"level":"info","ts":"2025-02-18T15:31:36.970621Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7177}
{"level":"info","ts":"2025-02-18T15:31:36.976583Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7177,"took":"5.512509ms","hash":1454153220,"current-db-size-bytes":4145152,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-18T15:31:36.976711Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1454153220,"revision":7177,"compact-revision":6897}
{"level":"info","ts":"2025-02-18T15:34:47.702234Z","caller":"traceutil/trace.go:171","msg":"trace[1258160091] transaction","detail":"{read_only:false; response_revision:7615; number_of_response:1; }","duration":"164.867596ms","start":"2025-02-18T15:34:47.515240Z","end":"2025-02-18T15:34:47.680108Z","steps":["trace[1258160091] 'process raft request'  (duration: 164.649144ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T15:35:00.763494Z","caller":"traceutil/trace.go:171","msg":"trace[559530805] transaction","detail":"{read_only:false; response_revision:7620; number_of_response:1; }","duration":"127.120439ms","start":"2025-02-18T15:35:00.636344Z","end":"2025-02-18T15:35:00.763464Z","steps":["trace[559530805] 'process raft request'  (duration: 126.923991ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T15:35:01.180609Z","caller":"traceutil/trace.go:171","msg":"trace[948219937] transaction","detail":"{read_only:false; response_revision:7621; number_of_response:1; }","duration":"182.838675ms","start":"2025-02-18T15:35:00.997742Z","end":"2025-02-18T15:35:01.180581Z","steps":["trace[948219937] 'process raft request'  (duration: 182.510334ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T15:35:01.382510Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T15:35:00.997717Z","time spent":"306.535591ms","remote":"127.0.0.1:34396","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":485,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" mod_revision:7613 > success:<request_put:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" value_size:426 >> failure:<request_range:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" > >"}


==> etcd [75fe39140d4c] <==
{"level":"warn","ts":"2025-02-19T08:16:31.356209Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-19T08:16:31.356275Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-02-19T08:16:31.356316Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-02-19T08:16:31.356325Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-02-19T08:16:31.356330Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-19T08:16:31.356350Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-19T08:16:31.358179Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-02-19T08:16:31.358789Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-02-19T08:16:31.372407Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"13.230339ms"}
{"level":"info","ts":"2025-02-19T08:16:31.411268Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-02-19T08:16:31.431145Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":9137}
{"level":"info","ts":"2025-02-19T08:16:31.431575Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-02-19T08:16:31.431619Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2025-02-19T08:16:31.431635Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 9137, applied: 0, lastindex: 9137, lastterm: 3]"}
{"level":"warn","ts":"2025-02-19T08:16:31.438433Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-19T08:16:31.439913Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":7177}
{"level":"info","ts":"2025-02-19T08:16:31.441566Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":7636}
{"level":"info","ts":"2025-02-19T08:16:31.444243Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-19T08:16:31.447362Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-02-19T08:16:31.447811Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-19T08:16:31.447873Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-02-19T08:16:31.448729Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-02-19T08:16:31.448981Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-19T08:16:31.449059Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-19T08:16:31.449145Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-19T08:16:31.449200Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-19T08:16:31.449182Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-19T08:16:31.449227Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-19T08:16:31.449236Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-19T08:16:31.449142Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-19T08:16:31.453155Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-19T08:16:31.453468Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-19T08:16:31.453501Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-19T08:16:31.453624Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-19T08:16:31.453638Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-19T08:16:32.432556Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2025-02-19T08:16:32.432612Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2025-02-19T08:16:32.432626Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-02-19T08:16:32.432637Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2025-02-19T08:16:32.432642Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-02-19T08:16:32.432649Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2025-02-19T08:16:32.432657Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-02-19T08:16:32.435539Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-19T08:16:32.435590Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-19T08:16:32.435571Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-19T08:16:32.436966Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-19T08:16:32.437049Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-19T08:16:32.437067Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-19T08:16:32.436998Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-19T08:16:32.437881Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-19T08:16:32.438034Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-19T08:16:34.108187Z","caller":"traceutil/trace.go:171","msg":"trace[556297291] transaction","detail":"{read_only:false; number_of_response:0; response_revision:7637; }","duration":"107.624282ms","start":"2025-02-19T08:16:34.000548Z","end":"2025-02-19T08:16:34.108172Z","steps":["trace[556297291] 'compare'  (duration: 88.961652ms)"],"step_count":1}
{"level":"info","ts":"2025-02-19T08:16:34.109002Z","caller":"traceutil/trace.go:171","msg":"trace[1764639135] linearizableReadLoop","detail":"{readStateIndex:9141; appliedIndex:9140; }","duration":"102.938771ms","start":"2025-02-19T08:16:34.006050Z","end":"2025-02-19T08:16:34.108989Z","steps":["trace[1764639135] 'read index received'  (duration: 12.754915ms)","trace[1764639135] 'applied index is now lower than readState.Index'  (duration: 90.183297ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-19T08:16:34.109262Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.187655ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-19T08:16:34.109389Z","caller":"traceutil/trace.go:171","msg":"trace[493405182] range","detail":"{range_begin:/registry/limitranges; range_end:; response_count:0; response_revision:7639; }","duration":"103.352211ms","start":"2025-02-19T08:16:34.006026Z","end":"2025-02-19T08:16:34.109379Z","steps":["trace[493405182] 'agreement among raft nodes before linearized reading'  (duration: 103.158413ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-19T08:16:34.109898Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.692671ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-02-19T08:16:34.109932Z","caller":"traceutil/trace.go:171","msg":"trace[307605584] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:7639; }","duration":"103.743444ms","start":"2025-02-19T08:16:34.006182Z","end":"2025-02-19T08:16:34.109926Z","steps":["trace[307605584] 'agreement among raft nodes before linearized reading'  (duration: 103.639831ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-19T08:16:34.110071Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.584624ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-19T08:16:34.110090Z","caller":"traceutil/trace.go:171","msg":"trace[764396694] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7639; }","duration":"100.622932ms","start":"2025-02-19T08:16:34.009463Z","end":"2025-02-19T08:16:34.110086Z","steps":["trace[764396694] 'agreement among raft nodes before linearized reading'  (duration: 100.59375ms)"],"step_count":1}
{"level":"info","ts":"2025-02-19T08:16:37.133157Z","caller":"traceutil/trace.go:171","msg":"trace[1225451625] transaction","detail":"{read_only:false; response_revision:7664; number_of_response:1; }","duration":"114.306221ms","start":"2025-02-19T08:16:37.018830Z","end":"2025-02-19T08:16:37.133136Z","steps":["trace[1225451625] 'process raft request'  (duration: 77.431636ms)","trace[1225451625] 'compare'  (duration: 36.748105ms)"],"step_count":2}


==> kernel <==
 08:24:33 up 14 min,  0 users,  load average: 0.52, 0.52, 0.44
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [1823965137bb] <==
I0218 13:25:42.395210       1 shared_informer.go:320] Caches are synced for configmaps
I0218 13:25:42.395236       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0218 13:25:42.395459       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0218 13:25:42.396360       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0218 13:25:42.396814       1 cache.go:39] Caches are synced for LocalAvailability controller
I0218 13:25:42.397911       1 cache.go:39] Caches are synced for autoregister controller
I0218 13:25:42.398012       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0218 13:25:42.398017       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0218 13:25:42.398966       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0218 13:25:42.400123       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0218 13:25:42.595143       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0218 13:25:43.096645       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0218 13:25:43.599666       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0218 13:25:44.025186       1 controller.go:615] quota admission added evaluator for: endpoints
E0218 13:25:48.225206       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0218 13:25:48.225223       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0218 13:25:48.225247       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.98.28.41:443: connect: no route to host
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0218 13:25:48.226265       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0218 13:25:48.231425       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: no route to host" logger="UnhandledError"
I0218 13:25:49.043825       1 controller.go:615] quota admission added evaluator for: serviceaccounts
W0218 13:25:49.227375       1 handler_proxy.go:99] no RequestInfo found in the context
E0218 13:25:49.227428       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0218 13:25:49.228582       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0218 13:25:50.534667       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0218 13:25:50.585010       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0218 13:25:51.345494       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: no route to host" logger="UnhandledError"
E0218 13:25:51.345494       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.98.28.41:443: connect: no route to host
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0218 13:25:51.345556       1 handler_proxy.go:99] no RequestInfo found in the context
E0218 13:25:51.345577       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0218 13:25:51.347097       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0218 13:25:51.353094       1 handler_proxy.go:99] no RequestInfo found in the context
E0218 13:25:51.353160       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0218 13:25:52.347414       1 handler_proxy.go:99] no RequestInfo found in the context
W0218 13:25:52.347504       1 handler_proxy.go:99] no RequestInfo found in the context
E0218 13:25:52.347530       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0218 13:25:52.347547       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0218 13:25:52.348634       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0218 13:25:52.348749       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0218 13:26:48.305397       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: connection refused" logger="UnhandledError"
W0218 13:26:48.305423       1 handler_proxy.go:99] no RequestInfo found in the context
E0218 13:26:48.305522       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0218 13:26:48.313621       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager


==> kube-apiserver [63ed5b9aefab] <==
I0219 08:16:34.826872       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0219 08:16:35.023396       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0219 08:16:39.002356       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0219 08:16:39.027201       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
I0219 08:16:41.220683       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0219 08:16:41.319961       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0219 08:16:41.470105       1 controller.go:615] quota admission added evaluator for: endpoints
I0219 08:16:41.470105       1 controller.go:615] quota admission added evaluator for: endpoints
I0219 08:16:41.470105       1 controller.go:615] quota admission added evaluator for: endpoints
E0219 08:16:42.152567       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: no route to host" logger="UnhandledError"
E0219 08:16:42.226367       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0219 08:16:42.227472       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0219 08:16:49.586442       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.98.28.41:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0219 08:16:49.586905       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.98.28.41:443: connect: connection refused
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0219 08:16:49.588124       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0219 08:16:49.588194       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:16:49.588211       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0219 08:16:50.588501       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:16:50.588589       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0219 08:16:50.589658       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:16:50.589719       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0219 08:16:50.589667       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0219 08:16:50.591014       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0219 08:17:43.497517       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:17:43.497584       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0219 08:17:43.497585       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: connection refused" logger="UnhandledError"
E0219 08:17:43.498972       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.28.41:443: connect: connection refused" logger="UnhandledError"
W0219 08:17:44.499752       1 handler_proxy.go:99] no RequestInfo found in the context
W0219 08:17:44.499787       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:17:44.499814       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0219 08:17:44.499847       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0219 08:17:44.501079       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0219 08:17:44.501150       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0219 08:17:48.510722       1 handler_proxy.go:99] no RequestInfo found in the context
E0219 08:17:48.510766       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.28.41:443/apis/metrics.k8s.io/v1beta1\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
E0219 08:17:48.510779       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0219 08:17:48.517528       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0219 08:18:52.948699       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-service" clusterIPs={"IPv4":"10.111.60.185"}
I0219 08:18:58.674008       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.107.117.14"}


==> kube-controller-manager [8809afb66cc9] <==
I0219 08:16:41.168721       1 shared_informer.go:320] Caches are synced for stateful set
I0219 08:16:41.172007       1 shared_informer.go:320] Caches are synced for resource quota
I0219 08:16:41.173293       1 shared_informer.go:320] Caches are synced for namespace
I0219 08:16:41.174593       1 shared_informer.go:320] Caches are synced for garbage collector
I0219 08:16:41.174629       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0219 08:16:41.174634       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0219 08:16:41.177008       1 shared_informer.go:320] Caches are synced for persistent volume
I0219 08:16:41.179541       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0219 08:16:41.184675       1 shared_informer.go:320] Caches are synced for crt configmap
I0219 08:16:41.185980       1 shared_informer.go:320] Caches are synced for garbage collector
I0219 08:16:41.226436       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="108.284557ms"
I0219 08:16:41.226543       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="51.903µs"
I0219 08:16:41.229135       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="111.023033ms"
I0219 08:16:41.229151       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="111.002948ms"
I0219 08:16:41.229232       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="56.714µs"
I0219 08:16:41.229240       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="33.271µs"
I0219 08:17:10.265844       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="8.043968ms"
I0219 08:17:10.265931       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="47.499µs"
I0219 08:17:12.057652       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="6.814816ms"
I0219 08:17:12.057748       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="38.931µs"
E0219 08:17:12.478013       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0219 08:17:12.493707       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0219 08:17:15.847227       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="43.089µs"
I0219 08:17:21.047257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="9.902298ms"
I0219 08:17:21.047567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="49.735µs"
I0219 08:17:32.697351       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="8.631849ms"
I0219 08:17:32.697601       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="80.16µs"
I0219 08:17:43.494552       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="6.668764ms"
I0219 08:17:43.494632       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="41.019µs"
E0219 08:17:43.745541       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0219 08:17:43.760208       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0219 08:18:52.943915       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="18.688008ms"
I0219 08:18:52.954817       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="10.850515ms"
I0219 08:18:52.954896       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="30.925µs"
I0219 08:18:52.965454       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="33.185µs"
I0219 08:18:58.665483       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="15.139954ms"
I0219 08:18:58.690221       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="24.688418ms"
I0219 08:18:58.690298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="38.316µs"
I0219 08:18:58.690389       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="82.502µs"
I0219 08:19:10.711861       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="41.457µs"
I0219 08:19:21.453264       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="37.807µs"
I0219 08:19:25.817233       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="30.449µs"
I0219 08:19:37.120956       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0219 08:19:41.993882       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="44.132µs"
I0219 08:19:54.006458       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="64.98µs"
I0219 08:20:07.299298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="30.45µs"
I0219 08:20:09.297355       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="40.026µs"
I0219 08:20:21.296664       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="36.391µs"
I0219 08:20:38.705379       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="38.548µs"
I0219 08:20:49.705386       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="34.492µs"
I0219 08:20:49.716458       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="37.423µs"
I0219 08:21:01.707085       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="45.367µs"
I0219 08:21:40.893788       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="61.333µs"
I0219 08:21:55.891161       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="30.794µs"
I0219 08:21:56.887932       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="35.824µs"
I0219 08:22:07.894038       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="38.464µs"
I0219 08:23:31.632752       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="89.333µs"
I0219 08:23:46.115968       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cccf8b6d8" duration="91.81µs"
I0219 08:23:52.117658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="100.731µs"
I0219 08:24:04.114772       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-6bb4795f54" duration="110.328µs"


==> kube-controller-manager [b0687f5ab366] <==
I0218 13:25:50.238647       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:25:50.255926       1 shared_informer.go:320] Caches are synced for resource quota
I0218 13:25:50.266111       1 shared_informer.go:320] Caches are synced for HPA
I0218 13:25:50.268354       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0218 13:25:50.271676       1 shared_informer.go:320] Caches are synced for PVC protection
I0218 13:25:50.273856       1 shared_informer.go:320] Caches are synced for crt configmap
I0218 13:25:50.277051       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0218 13:25:50.279512       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0218 13:25:50.281836       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0218 13:25:50.281987       1 shared_informer.go:320] Caches are synced for persistent volume
I0218 13:25:50.282011       1 shared_informer.go:320] Caches are synced for attach detach
I0218 13:25:50.282038       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0218 13:25:50.282065       1 shared_informer.go:320] Caches are synced for GC
I0218 13:25:50.282136       1 shared_informer.go:320] Caches are synced for daemon sets
I0218 13:25:50.282207       1 shared_informer.go:320] Caches are synced for deployment
I0218 13:25:50.282786       1 shared_informer.go:320] Caches are synced for expand
I0218 13:25:50.285702       1 shared_informer.go:320] Caches are synced for resource quota
I0218 13:25:50.286793       1 shared_informer.go:320] Caches are synced for ephemeral
I0218 13:25:50.287921       1 shared_informer.go:320] Caches are synced for namespace
I0218 13:25:50.289061       1 shared_informer.go:320] Caches are synced for TTL after finished
I0218 13:25:50.297288       1 shared_informer.go:320] Caches are synced for garbage collector
I0218 13:25:50.539286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="305.645661ms"
I0218 13:25:50.539428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="95.938µs"
I0218 13:25:50.542022       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="308.48097ms"
I0218 13:25:50.542087       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="24.227µs"
I0218 13:25:50.542140       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="308.524588ms"
I0218 13:25:50.542193       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="20.769µs"
I0218 13:25:55.568715       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="10.527887ms"
I0218 13:25:55.568837       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="47.22µs"
I0218 13:25:55.641456       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="62.598µs"
I0218 13:25:55.837639       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="8.380044ms"
I0218 13:25:55.837740       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="52.434µs"
E0218 13:26:23.019217       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0218 13:26:23.031873       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0218 13:26:48.292830       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="7.421469ms"
I0218 13:26:48.292993       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="47.811µs"
I0218 13:27:29.023354       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:33:02.511975       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:38:35.333186       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:44:42.343677       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:50:15.275458       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 13:55:51.887496       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:01:25.877206       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:06:58.352124       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:12:34.038123       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:18:04.479275       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:23:40.409894       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:29:14.011062       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:31:50.333312       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-k9m2x" approvedExpiration="1h0m0s"
I0218 14:34:47.122926       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:40:20.371480       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:45:53.714394       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:51:29.905077       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 14:57:02.951025       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:02:36.774773       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:08:04.511323       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:13:36.642896       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:19:12.841371       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:24:47.186769       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 15:30:19.549465       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [5d5f75713c91] <==
I0218 13:25:38.009161       1 server_linux.go:66] "Using iptables proxy"
I0218 13:25:42.401356       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0218 13:25:42.401434       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0218 13:25:42.805519       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0218 13:25:42.805585       1 server_linux.go:170] "Using iptables Proxier"
I0218 13:25:42.812113       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0218 13:25:42.820387       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0218 13:25:42.825567       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0218 13:25:42.825648       1 server.go:497] "Version info" version="v1.32.0"
I0218 13:25:42.825662       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0218 13:25:42.893843       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0218 13:25:42.918834       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0218 13:25:42.919741       1 config.go:199] "Starting service config controller"
I0218 13:25:42.919763       1 shared_informer.go:313] Waiting for caches to sync for service config
I0218 13:25:42.919788       1 config.go:105] "Starting endpoint slice config controller"
I0218 13:25:42.919793       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0218 13:25:42.920215       1 config.go:329] "Starting node config controller"
I0218 13:25:42.920221       1 shared_informer.go:313] Waiting for caches to sync for node config
I0218 13:25:43.094077       1 shared_informer.go:320] Caches are synced for node config
I0218 13:25:43.094115       1 shared_informer.go:320] Caches are synced for service config
I0218 13:25:43.094125       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [73939e8abd31] <==
I0219 08:16:37.701993       1 server_linux.go:66] "Using iptables proxy"
I0219 08:16:38.422716       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0219 08:16:38.422780       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0219 08:16:38.612036       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0219 08:16:38.612094       1 server_linux.go:170] "Using iptables Proxier"
I0219 08:16:38.696481       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0219 08:16:38.709855       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0219 08:16:38.718939       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0219 08:16:38.719040       1 server.go:497] "Version info" version="v1.32.0"
I0219 08:16:38.719063       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0219 08:16:38.726672       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0219 08:16:38.733085       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0219 08:16:38.733937       1 config.go:199] "Starting service config controller"
I0219 08:16:38.733963       1 shared_informer.go:313] Waiting for caches to sync for service config
I0219 08:16:38.734000       1 config.go:105] "Starting endpoint slice config controller"
I0219 08:16:38.734007       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0219 08:16:38.734007       1 config.go:329] "Starting node config controller"
I0219 08:16:38.734022       1 shared_informer.go:313] Waiting for caches to sync for node config
I0219 08:16:38.834771       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0219 08:16:38.834826       1 shared_informer.go:320] Caches are synced for node config
I0219 08:16:38.834844       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [1ddd92aef2c1] <==
I0219 08:16:32.311188       1 serving.go:386] Generated self-signed cert in-memory
W0219 08:16:33.928384       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0219 08:16:33.928414       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0219 08:16:33.928423       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0219 08:16:33.928430       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0219 08:16:34.099939       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0219 08:16:34.099982       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0219 08:16:34.102175       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0219 08:16:34.102234       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0219 08:16:34.102673       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0219 08:16:34.102698       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0219 08:16:34.202995       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [e542b8495274] <==
I0218 13:25:40.335587       1 serving.go:386] Generated self-signed cert in-memory
W0218 13:25:42.294561       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0218 13:25:42.300730       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0218 13:25:42.300776       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0218 13:25:42.300789       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0218 13:25:42.397332       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0218 13:25:42.397359       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0218 13:25:42.414068       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0218 13:25:42.414165       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0218 13:25:42.414204       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0218 13:25:42.414220       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0218 13:25:42.514671       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 19 08:19:41 minikube kubelet[1796]: E0219 08:19:41.878764    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbnvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-7cccf8b6d8-x4xbh_default(10360b8a-59b7-4039-a39a-48e7115ab8ee): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Feb 19 08:19:41 minikube kubelet[1796]: E0219 08:19:41.879894    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:19:53 minikube kubelet[1796]: E0219 08:19:53.989327    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:19:57 minikube kubelet[1796]: E0219 08:19:57.000507    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:19:57 minikube kubelet[1796]: E0219 08:19:57.000550    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:19:57 minikube kubelet[1796]: E0219 08:19:57.000627    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-6bb4795f54-fzpvx_default(269b8f1f-ad70-43b2-b4f8-71015be5c985): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp 3.94.224.37:443: i/o timeout" logger="UnhandledError"
Feb 19 08:19:57 minikube kubelet[1796]: E0219 08:19:57.001755    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp 3.94.224.37:443: i/o timeout\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:20:09 minikube kubelet[1796]: E0219 08:20:09.289505    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp 3.94.224.37:443: i/o timeout\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:20:22 minikube kubelet[1796]: E0219 08:20:22.364643    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" image="mongo:5.0"
Feb 19 08:20:22 minikube kubelet[1796]: E0219 08:20:22.364689    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" image="mongo:5.0"
Feb 19 08:20:22 minikube kubelet[1796]: E0219 08:20:22.364856    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbnvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-7cccf8b6d8-x4xbh_default(10360b8a-59b7-4039-a39a-48e7115ab8ee): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" logger="UnhandledError"
Feb 19 08:20:22 minikube kubelet[1796]: E0219 08:20:22.366050    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:20:38 minikube kubelet[1796]: E0219 08:20:38.696984    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:20:38 minikube kubelet[1796]: E0219 08:20:38.784668    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:20:38 minikube kubelet[1796]: E0219 08:20:38.784718    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:20:38 minikube kubelet[1796]: E0219 08:20:38.784827    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-6bb4795f54-fzpvx_default(269b8f1f-ad70-43b2-b4f8-71015be5c985): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Feb 19 08:20:38 minikube kubelet[1796]: E0219 08:20:38.786006    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:20:49 minikube kubelet[1796]: E0219 08:20:49.696950    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:20:49 minikube kubelet[1796]: E0219 08:20:49.697001    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:21:00 minikube kubelet[1796]: E0219 08:21:00.697432    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:21:01 minikube kubelet[1796]: E0219 08:21:01.696981    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:21:14 minikube kubelet[1796]: E0219 08:21:14.180231    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:21:28 minikube kubelet[1796]: E0219 08:21:28.225757    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="mongo:5.0"
Feb 19 08:21:28 minikube kubelet[1796]: E0219 08:21:28.225803    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="mongo:5.0"
Feb 19 08:21:28 minikube kubelet[1796]: E0219 08:21:28.225879    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbnvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-7cccf8b6d8-x4xbh_default(10360b8a-59b7-4039-a39a-48e7115ab8ee): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Feb 19 08:21:28 minikube kubelet[1796]: E0219 08:21:28.226988    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:21:40 minikube kubelet[1796]: E0219 08:21:40.881447    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:21:43 minikube kubelet[1796]: E0219 08:21:43.963832    1796 container_manager_linux.go:516] "Failed to find cgroups of kubelet" err="cpu and memory cgroup hierarchy not unified.  cpu: /docker/0bddbfcdc0158410aaf58576c9511edc6fc64abaa4b1f770b0eeb7373d8bf508/docker/0bddbfcdc0158410aaf58576c9511edc6fc64abaa4b1f770b0eeb7373d8bf508/system.slice/kubelet.service, memory: /docker/0bddbfcdc0158410aaf58576c9511edc6fc64abaa4b1f770b0eeb7373d8bf508/system.slice/kubelet.service"
Feb 19 08:21:45 minikube kubelet[1796]: E0219 08:21:45.892982    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:21:45 minikube kubelet[1796]: E0219 08:21:45.893090    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:21:45 minikube kubelet[1796]: E0219 08:21:45.893178    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-6bb4795f54-fzpvx_default(269b8f1f-ad70-43b2-b4f8-71015be5c985): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Feb 19 08:21:45 minikube kubelet[1796]: E0219 08:21:45.894362    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:21:55 minikube kubelet[1796]: E0219 08:21:55.881544    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:21:56 minikube kubelet[1796]: E0219 08:21:56.880623    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:22:07 minikube kubelet[1796]: E0219 08:22:07.885527    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:22:11 minikube kubelet[1796]: E0219 08:22:11.328402    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:22:23 minikube kubelet[1796]: E0219 08:22:23.326588    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:22:23 minikube kubelet[1796]: E0219 08:22:23.326588    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:22:36 minikube kubelet[1796]: E0219 08:22:36.323597    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:22:37 minikube kubelet[1796]: E0219 08:22:37.323413    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:22:51 minikube kubelet[1796]: E0219 08:22:51.914982    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:22:52 minikube kubelet[1796]: E0219 08:22:52.913672    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:23:07 minikube kubelet[1796]: E0219 08:23:07.913301    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:23:19 minikube kubelet[1796]: E0219 08:23:19.678533    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="mongo:5.0"
Feb 19 08:23:19 minikube kubelet[1796]: E0219 08:23:19.678606    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="mongo:5.0"
Feb 19 08:23:19 minikube kubelet[1796]: E0219 08:23:19.678716    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbnvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-7cccf8b6d8-x4xbh_default(10360b8a-59b7-4039-a39a-48e7115ab8ee): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" logger="UnhandledError"
Feb 19 08:23:19 minikube kubelet[1796]: E0219 08:23:19.679816    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:23:31 minikube kubelet[1796]: E0219 08:23:31.616593    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:23:39 minikube kubelet[1796]: E0219 08:23:39.679029    1796 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:23:39 minikube kubelet[1796]: E0219 08:23:39.679149    1796 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" image="nanajanashia/k8s-demo-app:v1.0"
Feb 19 08:23:39 minikube kubelet[1796]: E0219 08:23:39.679364    1796 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-6bb4795f54-fzpvx_default(269b8f1f-ad70-43b2-b4f8-71015be5c985): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded" logger="UnhandledError"
Feb 19 08:23:39 minikube kubelet[1796]: E0219 08:23:39.680759    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:23:46 minikube kubelet[1796]: E0219 08:23:46.092182    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:23:52 minikube kubelet[1796]: E0219 08:23:52.092702    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:23:57 minikube kubelet[1796]: E0219 08:23:57.092266    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:24:04 minikube kubelet[1796]: E0219 08:24:04.091715    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:24:10 minikube kubelet[1796]: E0219 08:24:10.095555    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:24:16 minikube kubelet[1796]: E0219 08:24:16.638169    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"
Feb 19 08:24:22 minikube kubelet[1796]: E0219 08:24:22.639376    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with ImagePullBackOff: \"Back-off pulling image \\\"mongo:5.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="default/mongo-deployment-7cccf8b6d8-x4xbh" podUID="10360b8a-59b7-4039-a39a-48e7115ab8ee"
Feb 19 08:24:27 minikube kubelet[1796]: E0219 08:24:27.639564    1796 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with ImagePullBackOff: \"Back-off pulling image \\\"nanajanashia/k8s-demo-app:v1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/webapp-deployment-6bb4795f54-fzpvx" podUID="269b8f1f-ad70-43b2-b4f8-71015be5c985"


==> kubernetes-dashboard [7ce8a461c453] <==
2025/02/19 08:16:38 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00055fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000142300)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2025/02/19 08:16:38 Using namespace: kubernetes-dashboard
2025/02/19 08:16:38 Using in-cluster config to connect to apiserver
2025/02/19 08:16:38 Using secret token for csrf signing
2025/02/19 08:16:38 Initializing csrf token from kubernetes-dashboard-csrf secret


==> kubernetes-dashboard [b9797d26291f] <==
2025/02/19 08:18:03 Getting list of all replication controllers in the cluster
2025/02/19 08:18:03 [2025-02-19T08:18:03Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:03 [2025-02-19T08:18:03Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:03 Getting list of all pet sets in the cluster
2025/02/19 08:18:03 [2025-02-19T08:18:03Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:03 [2025-02-19T08:18:03Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all cron jobs in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of namespaces
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all deployments in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all jobs in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all pods in the cluster
2025/02/19 08:18:08 Getting pod metrics
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all replication controllers in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all replica sets in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all pet sets in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all cron jobs in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all deployments in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all jobs in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of namespaces
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all pods in the cluster
2025/02/19 08:18:08 Getting pod metrics
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all replica sets in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all replication controllers in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/02/19 08:18:08 Getting list of all pet sets in the cluster
2025/02/19 08:18:08 [2025-02-19T08:18:08Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [37f596c04238] <==
I0219 08:16:37.533994       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0219 08:17:08.925433       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [3cfdb312087c] <==
I0219 08:17:20.911785       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0219 08:17:20.921041       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0219 08:17:20.921546       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0219 08:17:39.577887       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0219 08:17:39.578006       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_da5abb06-e6ab-4fe0-a0d0-13f6461b64e8!
I0219 08:17:39.578007       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"9af82c0c-bb78-4499-9b70-7bf55759cf1e", APIVersion:"v1", ResourceVersion:"7847", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_da5abb06-e6ab-4fe0-a0d0-13f6461b64e8 became leader
I0219 08:17:39.679158       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_da5abb06-e6ab-4fe0-a0d0-13f6461b64e8!

